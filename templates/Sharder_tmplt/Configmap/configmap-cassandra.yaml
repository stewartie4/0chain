apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    {{- include "0helm-0chain.labels" $ | nindent 6 }}-cassandra-yaml
  creationTimestamp: null
  name: cassandra-yaml
data:
  cassandra.yaml: "# Cassandra storage config YAML\n\n# NOTE:\n#   See http://wiki.apache.org/cassandra/StorageConfiguration
    for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name
    of the cluster. This is mainly used to prevent machines in\n# one logical cluster
    from joining another.\ncluster_name: 'zerochain'\n\n# This defines the number
    of tokens randomly assigned to this node on the ring\n# The more tokens, relative
    to other nodes, the larger the proportion of data\n# that this node will store.
    You probably want all nodes to have the same number\n# of tokens assuming they
    have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra
    will use the default of 1 token for legacy compatibility,\n# and will use the
    initial_token as described below.\n#\n# Specifying initial_token will override
    this setting on the node's initial start,\n# on subsequent starts, this setting
    will apply even if initial token is set.\n#\n# If you already have a cluster with
    1 token per node, and wish to migrate to \n# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations\nnum_tokens:
    256\n\n# Triggers automatic allocation of num_tokens tokens for this node. The
    allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated
    load over\n# the nodes in the datacenter for the replication strategy used by
    the specified\n# keyspace.\n#\n# The load assigned to each node will be close
    to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n#
    allocate_tokens_for_keyspace: KEYSPACE\n\n# initial_token allows you to specify
    tokens manually.  While you can use it with\n# vnodes (num_tokens > 1, above)
    -- in which case you should provide a \n# comma-separated list -- it's primarily
    used when adding nodes to legacy clusters \n# that do not have vnodes enabled.\n#
    initial_token:\n\n# See http://wiki.apache.org/cassandra/HintedHandoff\n# May
    either be \"true\" or \"false\" to enable globally\nhinted_handoff_enabled: true\n\n#
    When hinted_handoff_enabled is true, a black list of data centers that will not\n#
    perform hinted handoff\n# hinted_handoff_disabled_datacenters:\n#    - DC1\n#
    \   - DC2\n\n# this defines the maximum amount of time a dead host will have hints\n#
    generated.  After it has been dead this long, new hints for it will not be\n#
    created until it has been seen alive and gone down again.\nmax_hint_window_in_ms:
    10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.
    \ This will be\n# reduced proportionally to the number of nodes in the cluster.
    \ (If there\n# are two nodes in the cluster, each delivery thread will use the
    maximum\n# rate; if there are three, each will throttle to half of the maximum,\n#
    since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb:
    1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing
    this number when you have multi-dc deployments, since\n# cross-dc handoff tends
    to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should
    store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\n#
    hints_directory: /usr/local/homebrew/var/lib/cassandra/hints\n\n# How often hints
    should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms:
    10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb:
    128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will
    be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#
    \  - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle
    in KBs per second, total. This will be\n# reduced proportionally to the number
    of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication
    backend, implementing IAuthenticator; used to identify users\n# Out of the box,
    Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n#
    - AllowAllAuthenticator performs no checks - set it to disable authentication.\n#
    - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users.
    It keeps usernames and hashed passwords in system_auth.roles table.\n#   Please
    increase system_auth keyspace replication factor if you use this authenticator.\n#
    \  If using PasswordAuthenticator, CassandraRoleManager must also be used (see
    below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing
    IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra
    provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n#
    - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n#
    - CassandraAuthorizer stores permissions in system_auth.role_permissions table.
    Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer:
    AllowAllAuthorizer\n\n# Part of the Authentication & Authorization backend, implementing
    IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out
    of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n#
    which stores role information in the system_auth keyspace. Most functions of the\n#
    IRoleManager require an authenticated login, so unless the configured IAuthenticator\n#
    actually implements authentication, most of this functionality will be unavailable.\n#\n#
    - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#
    \  increase system_auth keyspace replication factor if you use this role manager.\nrole_manager:
    CassandraRoleManager\n\n# Validity period for roles cache (fetching granted roles
    can be an expensive\n# operation depending on the role manager, CassandraRoleManager
    is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser
    and\n# after the period specified here, become eligible for (async) reload.\n#
    Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically
    for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for
    roles cache (if enabled).\n# After this interval, cache entries become eligible
    for refresh. Upon next\n# access, an async reload is scheduled and the old value
    returned until it\n# completes. If roles_validity_in_ms is non-zero, then this
    must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms:
    2000\n\n# Validity period for permissions cache (fetching permissions can be an\n#
    expensive operation depending on the authorizer, CassandraAuthorizer is\n# one
    example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically
    for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval
    for permissions cache (if enabled).\n# After this interval, cache entries become
    eligible for refresh. Upon next\n# access, an async reload is scheduled and the
    old value returned until it\n# completes. If permissions_validity_in_ms is non-zero,
    then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n#
    permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache.
    This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation
    of IAuthenticator. If\n# another IAuthenticator implementation is configured,
    this cache will not\n# be automatically used and so the following settings will
    have no effect.\n# Please note, credentials are cached in their encrypted form,
    so while\n# activating this cache may reduce the number of queries made to the\n#
    underlying table, it may not  bring a significant reduction in the\n# latency
    of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable
    credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for
    credentials cache (if enabled).\n# After this interval, cache entries become eligible
    for refresh. Upon next\n# access, an async reload is scheduled and the old value
    returned until it\n# completes. If credentials_validity_in_ms is non-zero, then
    this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n#
    credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for
    distributing groups of rows (by\n# partition key) across nodes in the cluster.
    \ You should leave this\n# alone for new clusters.  The partitioner can NOT be
    changed without\n# reloading all data, so when upgrading you should set this to
    the\n# same partitioner you were already using.\n#\n# Besides Murmur3Partitioner,
    partitioners included for backwards\n# compatibility include RandomPartitioner,
    ByteOrderedPartitioner, and\n# OrderPreservingPartitioner.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n#
    Directories where Cassandra should store data on disk.  Cassandra\n# will spread
    data evenly across them, subject to the granularity of\n# the configured compaction
    strategy.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\n#
    data_file_directories:\n#     - /usr/local/homebrew/var/lib/cassandra/data\n\n#
    commit log.  when running on magnetic HDD, this should be a\n# separate spindle
    than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\n#
    commitlog_directory: /usr/local/homebrew/var/lib/cassandra/commitlog\n\n# Enable
    / disable CDC functionality on a per-node basis. This modifies the logic used\n#
    for write path allocation rejection (standard: never reject. cdc: reject Mutation\n#
    containing a CDC-enabled table if at space limit in cdc_raw_directory).\ncdc_enabled:
    false\n\n# CommitLogSegments are moved to this directory on flush if cdc_enabled:
    true and the\n# segment contains mutations for a CDC-enabled table. This should
    be placed on a\n# separate spindle than the data directories. If not set, the
    default directory is\n# $CASSANDRA_HOME/data/cdc_raw.\n# cdc_raw_directory: /usr/local/homebrew/var/lib/cassandra/cdc_raw\n\n#
    Policy for data disk failures:\n#\n# die\n#   shut down gossip and client transports
    and kill the JVM for any fs errors or\n#   single-sstable errors, so the node
    can be replaced.\n#\n# stop_paranoid\n#   shut down gossip and client transports
    even for single-sstable errors,\n#   kill the JVM for errors during startup.\n#\n#
    stop\n#   shut down gossip and client transports, leaving the node effectively
    dead, but\n#   can still be inspected via JMX, kill the JVM for errors during
    startup.\n#\n# best_effort\n#    stop using the failed disk and respond to requests
    based on\n#    remaining available sstables.  This means you WILL see obsolete\n#
    \   data at CL.ONE!\n#\n# ignore\n#    ignore fatal errors and let requests fail,
    as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# Policy for commit disk
    failures:\n#\n# die\n#   shut down gossip and Thrift and kill the JVM, so the
    node can be replaced.\n#\n# stop\n#   shut down gossip and Thrift, leaving the
    node effectively dead, but\n#   can still be inspected via JMX.\n#\n# stop_commit\n#
    \  shutdown the commit log, letting writes collect but\n#   continuing to service
    reads, as in pre-2.0.5 Cassandra\n#\n# ignore\n#   ignore fatal errors and let
    the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native
    protocol prepared statement cache\n#\n# Valid values are either \"auto\" (omitting
    the value) or a value greater 0.\n#\n# Note that specifying a too large value
    will result in long running GCs and possbily\n# out-of-memory errors. Keep the
    value at a small fraction of the heap.\n#\n# If you constantly see \"prepared
    statements discarded in the last minute because\n# cache limit reached\" messages,
    the first step is to investigate the root cause\n# of these messages and check
    whether prepared statements are used correctly -\n# i.e. use bind markers for
    variable parts.\n#\n# Do only change the default value, if you really have more
    prepared statements than\n# fit in the cache. In most cases it is not neccessary
    to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n#
    Default value (\"auto\") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n#
    Maximum size of the Thrift prepared statement cache\n#\n# If you do not use Thrift
    at all, it is safe to leave this value at \"auto\".\n#\n# See description of 'prepared_statements_cache_size_mb'
    above for more information.\n#\n# Default value (\"auto\") is 1/256th of the heap
    or 10MB, whichever is greater\nthrift_prepared_statements_cache_size_mb:\n\n#
    Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek
    and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key
    cache is fairly tiny for the amount of\n# time it saves, so it's worthwhile to
    use it at large numbers.\n# The row cache saves even more time, but must contain
    the entire row,\n# so it is extremely space-intensive. It's best to only use the\n#
    row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size,
    you may not get you hottest keys loaded on startup.\n#\n# Default value is empty
    to make it \"auto\" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key
    cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n#
    save the key cache. Caches are saved to saved_caches_directory as\n# specified
    in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds,
    and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving
    is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period:
    14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning
    all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation
    class name. Available implementations:\n#\n# org.apache.cassandra.cache.OHCProvider\n#
    \  Fully off-heap row cache implementation (default).\n#\n# org.apache.cassandra.cache.SerializingCacheProvider\n#
    \  This is the row cache implementation availabile\n#   in previous releases of
    Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n#
    Maximum size of the row cache in memory.\n# Please note that OHC cache implementation
    requires some additional off-heap memory to manage\n# the map structures and some
    in-flight memory during operations before/after cache entries can be\n# accounted
    against the cache capacity. This overhead is usually small compared to the whole
    capacity.\n# Do not specify more memory that the system can afford in the worst
    usual situation and leave some\n# headroom for OS block level cache. Do never
    allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb:
    0\n\n# Duration in seconds after which Cassandra should save the row cache.\n#
    Caches are saved to saved_caches_directory as specified in this configuration
    file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively
    cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive
    and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period:
    0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default),
    meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum
    size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter
    locks' contention for hot counter cells.\n# In case of RF = 1 a counter cache
    hit will cause Cassandra to skip the read before\n# write entirely. With RF >
    1 a counter cache hit will still help to reduce the duration\n# of the lock hold,
    helping with hot counter cell updates, but will not allow skipping\n# the read
    entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in
    memory, not the whole counter, so it's relatively cheap.\n#\n# NOTE: if you reduce
    the size, you may not get you hottest keys loaded on startup.\n#\n# Default value
    is empty to make it \"auto\" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable
    counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you
    should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in
    seconds after which Cassandra should\n# save the counter cache (keys only). Caches
    are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n#
    Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys
    from the counter cache to save\n# Disabled by default, meaning all keys are going
    to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set,
    the default directory is $CASSANDRA_HOME/data/saved_caches.\n# saved_caches_directory:
    /usr/local/homebrew/var/lib/cassandra/saved_caches\n\n# commitlog_sync may be
    either \"periodic\" or \"batch.\" \n# \n# When in batch mode, Cassandra won't
    ack writes until the commit log\n# has been fsynced to disk.  It will wait\n#
    commitlog_sync_batch_window_in_ms milliseconds between fsyncs.\n# This window
    should be kept short because the writer threads will\n# be unable to do extra
    work while waiting.  (You may need to increase\n# concurrent_writes for the same
    reason.)\n#\n# commitlog_sync: batch\n# commitlog_sync_batch_window_in_ms: 2\n#\n#
    the other option is \"periodic\" where writes may be acked immediately\n# and
    the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds.\ncommitlog_sync:
    periodic\ncommitlog_sync_period_in_ms: 10000\n\n# The size of the individual commitlog
    file segments.  A commitlog\n# segment may be archived, deleted, or recycled once
    all the data\n# in it (potentially from each columnfamily in the system) has been\n#
    flushed to sstables.\n#\n# The default size is 32, which is almost always fine,
    but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n#
    then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n#
    Max mutation size is also configurable via max_mutation_size_in_kb setting in\n#
    cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n#
    This should be positive and less than 2048.\n#\n# NOTE: If max_mutation_size_in_kb
    is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least
    twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb:
    32\n\n# Compression to apply to the commit log. If omitted, the commit log\n#
    will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n#
    commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#
    \        -\n\n# any class that implements the SeedProvider interface and has a\n#
    constructor that takes a Map<String, String> of parameters will do.\nseed_provider:\n
    \   # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes
    use this list of hosts to find each other and learn\n    # the topology of the
    ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name:
    org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          #
    seeds is actually a comma-delimited list of addresses.\n          # Ex: \"<ip1>,<ip2>,<ip3>\"\n
    \         - seeds: \"127.0.0.1\"\n\n# For workloads with more data than can fit
    in memory, Cassandra's\n# bottleneck will be reads that need to fetch data from\n#
    disk. \"concurrent_reads\" should be set to (16 * number_of_drives) in\n# order
    to allow the operations to enqueue low enough in the stack\n# that the OS and
    drives can reorder them. Same applies to\n# \"concurrent_counter_writes\", since
    counter writes read the current\n# values before incrementing and writing them
    back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n#
    number of \"concurrent_writes\" is dependent on the number of cores in\n# your
    system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes:
    32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there
    is a read involved, so this should\n# be limited by the less of concurrent reads
    or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory
    to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved
    for pooling buffers, the rest is used as an\n# cache that holds uncompressed sstable
    chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated
    off-heap,\n# so is in addition to the memory allocated for heap. The cache also
    has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the
    reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated
    when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate
    on or off heap when the sstable buffer\n# pool is exhausted, that is when it has
    exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not
    cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted:
    true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd
    (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy:
    ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting
    writes when the limit is exceeded until a flush completes,\n# and will trigger
    a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set
    both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb:
    2048\n\n# memtable_cleanup_threshold is deprecated. The default calculation\n#
    is the only reasonable choice. See the comments on  memtable_flush_writers\n#
    for more information.\n#\n# Ratio of occupied non-flushing memtable size to total
    permitted size\n# that will trigger a flush of the largest memtable. Larger mct
    will\n# mean larger flushes and hence less compaction, but also less concurrent\n#
    flush activity which can make it difficult to keep your disks fed\n# under heavy
    write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers
    + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates
    and manages memtable memory.\n# Options are:\n#\n# heap_buffers\n#   on heap nio
    buffers\n#\n# offheap_buffers\n#   off heap (direct) nio buffers\n#\n# offheap_objects\n#
    \   off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Total space
    to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra
    will flush every dirty CF\n# in the oldest segment and remove it.  So a small
    total commitlog space\n# will tend to cause more flush activity on less-active
    columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the
    total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n#
    This sets the number of memtable flush writer threads per disk\n# as well as the
    total number of memtables that can be flushed concurrently.\n# These are generally
    a combination of compute and IO bound.\n#\n# Memtable flushing is more CPU efficient
    than memtable ingest and a single thread\n# can keep up with the ingest rate of
    a whole server on a single fast disk\n# until it temporarily becomes IO bound
    under contention typically with compaction.\n# At that point you need multiple
    flush threads. At some point in the future\n# it may become CPU bound all the
    time.\n#\n# You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\n#
    metric which should be 0, but will be non-zero if threads are blocked waiting
    on flushing\n# to free memory.\n#\n# memtable_flush_writers defaults to two for
    a single data directory.\n# This means that two  memtables can be flushed concurrently
    to the single data directory.\n# If you have multiple data directories the default
    is one memtable flushing at a time\n# but the flush will use a thread per data
    directory so you will get two or more writers.\n#\n# Two is generally enough to
    flush on a fast disk [array] mounted as a single data directory.\n# Adding more
    flush writers will result in smaller more frequent flushes that introduce more\n#
    compaction overhead.\n#\n# There is a direct tradeoff between number of memtables
    that can be flushed concurrently\n# and flush size and frequency. More is not
    better you just need enough flush writers\n# to never stall waiting for flushing
    to free memory.\n#\n#memtable_flush_writers: 2\n\n# Total space to use for change-data-capture
    logs on disk.\n#\n# If space gets above this value, Cassandra will throw WriteTimeoutException\n#
    on Mutations including tables with CDC enabled. A CDCCompactor is responsible\n#
    for parsing the raw CDC logs and deleting them when parsing is completed.\n#\n#
    The default value is the min of 4096 mb and 1/8th of the total space\n# of the
    drive where cdc_raw_directory resides.\n# cdc_total_space_in_mb: 4096\n\n# When
    we hit our cdc_raw limit and the CDCCompactor is either running behind\n# or experiencing
    backpressure, we check at the following interval to see if any\n# new space for
    cdc-tracked tables has been made available. Default to 250ms\n# cdc_free_space_check_interval_ms:
    250\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n#
    empty, this will default to 5% of the heap size. If the memory usage of\n# all
    index summaries exceeds this limit, SSTables with low read rates will\n# shrink
    their index summaries in order to meet this limit.  However, this\n# is a best-effort
    process. In extreme conditions Cassandra may need to use\n# more than this amount
    of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries
    should be resampled.  This is done\n# periodically to redistribute memory from
    the fixed-size pool to sstables\n# proportional their recent read rates.  Setting
    to -1 will disable this\n# process, leaving existing index summaries at their
    current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether
    to, when doing sequential writing, fsync() at intervals in\n# order to force the
    operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty
    buffer flushing from\n# impacting read latencies. Almost always a good idea on
    SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb:
    10240\n\n# TCP port, for commands and data\n# For security reasons, you should
    not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n#
    SSL port, for encrypted communication.  Unused unless enabled in\n# encryption_options\n#
    For security reasons, you should not expose this port to the internet.  Firewall
    it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and
    tell other Cassandra nodes to connect to.\n# You _must_ change this if you want
    multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface,
    not both.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n#
    will always do the Right Thing _if_ the node is properly configured\n# (hostname,
    name resolution, etc), and the Right Thing is to use the\n# address associated
    with the hostname (it might not be).\n#\n# Setting listen_address to 0.0.0.0 is
    always wrong.\n#\nlisten_address: localhost\n\n# Set listen_address OR listen_interface,
    not both. Interfaces must correspond\n# to a single address, IP aliasing is not
    supported.\n# listen_interface: eth0\n\n# If you choose to specify the interface
    by name and the interface has an ipv4 and an ipv6 address\n# you can specify which
    should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n#
    address will be used. If true the first ipv6 address will be used. Defaults to
    false preferring\n# ipv4. If there is only one address it will be selected regardless
    of ipv4/ipv6.\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast
    to other Cassandra nodes\n# Leaving this blank will set it to the same value as
    listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical
    network interfaces, set this\n# to true to listen on broadcast_address in addition
    to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n#
    Ignore this property if the network configuration automatically\n# routes  between
    the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n#
    Internode authentication backend, implementing IInternodeAuthenticator;\n# used
    to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n#
    Whether to start the native transport server.\n# Please note that the address
    on which the native transport is bound is the\n# same as the rpc_address. The
    port however is different and specified below.\nstart_native_transport: true\n#
    port for the CQL native transport to listen for clients on\n# For security reasons,
    you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port:
    9042\n# Enabling native transport encryption in client_encryption_options allows
    you to either use\n# encryption for the standard port or to use a dedicated, additional
    port along with the unencrypted\n# standard native_transport_port.\n# Enabling
    client encryption and keeping native_transport_port_ssl disabled will use encryption\n#
    for native_transport_port. Setting native_transport_port_ssl to a different value\n#
    from native_transport_port will use encryption for native_transport_port_ssl while\n#
    keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n#
    The maximum threads for handling requests when the native transport is used.\n#
    This is similar to rpc_max_threads though the default differs slightly (and\n#
    there is no native_transport_min_threads, idle threads will always be stopped\n#
    after 30 seconds).\n# native_transport_max_threads: 128\n#\n# The maximum size
    of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid.
    The default is 256MB. If you're changing this parameter,\n# you may want to adjust
    max_value_size_in_mb accordingly. This should be positive and less than 2048.\n#
    native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent
    client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections:
    -1\n\n# The maximum number of concurrent client connections per source ip.\n#
    The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip:
    -1\n\n# Whether to start the thrift rpc server.\nstart_rpc: false\n\n# The address
    or interface to bind the Thrift RPC service and native transport\n# server to.\n#\n#
    Set rpc_address OR rpc_interface, not both.\n#\n# Leaving rpc_address blank has
    the same effect as on listen_address\n# (i.e. it will be based on the configured
    hostname of the node).\n#\n# Note that unlike listen_address, you can specify
    0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than
    0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.
    \ Firewall it if needed.\nrpc_address: localhost\n\n# Set rpc_address OR rpc_interface,
    not both. Interfaces must correspond\n# to a single address, IP aliasing is not
    supported.\n# rpc_interface: eth1\n\n# If you choose to specify the interface
    by name and the interface has an ipv4 and an ipv6 address\n# you can specify which
    should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address
    will be used. If true the first ipv6 address will be used. Defaults to false preferring\n#
    ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\n#
    rpc_interface_prefer_ipv6: false\n\n# port for Thrift to listen for clients on\nrpc_port:
    9160\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This
    cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n#
    rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be
    set.\n# broadcast_rpc_address: 1.2.3.4\n\n# enable or disable keepalive on rpc/native
    connections\nrpc_keepalive: true\n\n# Cassandra provides two out-of-the-box options
    for the RPC Server:\n#\n# sync\n#   One thread per thrift connection. For a very
    large number of clients, memory\n#   will be your limiting factor. On a 64 bit
    JVM, 180KB is the minimum stack size\n#   per thread, and that will correspond
    to your use of virtual memory (but physical memory\n#   may be limited depending
    on use of stack space).\n#\n# hsha\n#   Stands for \"half synchronous, half asynchronous.\"
    All thrift clients are handled\n#   asynchronously using a small number of threads
    that does not vary with the amount\n#   of thrift clients (and thus scales well
    to many clients). The rpc requests are still\n#   synchronous (one thread per
    active request). If hsha is selected then it is essential\n#   that rpc_max_threads
    is changed from the default value of unlimited.\n#\n# The default is sync because
    on Windows hsha is about 30% slower.  On Linux,\n# sync/hsha performance is about
    the same, with hsha of course using less memory.\n#\n# Alternatively,  can provide
    your own RPC server by providing the fully-qualified class name\n# of an o.a.c.t.TServerFactory
    that can create an instance of it.\nrpc_server_type: sync\n\n# Uncomment rpc_min|max_thread
    to set request pool size limits.\n#\n# Regardless of your choice of RPC server
    (see above), the number of maximum requests in the\n# RPC thread pool dictates
    how many concurrent requests are possible (but if you are using the sync\n# RPC
    server, it also dictates the number of clients that can be connected at all).\n#\n#
    The default is unlimited and thus provides no protection against clients overwhelming
    the server. You are\n# encouraged to set a maximum that makes sense for you in
    production, but do keep in mind that\n# rpc_max_threads represents the maximum
    number of client requests this server may execute concurrently.\n#\n# rpc_min_threads:
    16\n# rpc_max_threads: 2048\n\n# uncomment to set socket buffer sizes on rpc connections\n#
    rpc_send_buff_size_in_bytes:\n# rpc_recv_buff_size_in_bytes:\n\n# Uncomment to
    set socket buffer size for internode communication\n# Note that when setting this,
    the buffer size is limited by net.core.wmem_max\n# and when not setting it it
    is defined by net.ipv4.tcp_wmem\n# See also:\n# /proc/sys/net/core/wmem_max\n#
    /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n#
    and 'man tcp'\n# internode_send_buff_size_in_bytes:\n\n# Uncomment to set socket
    buffer size for internode communication\n# Note that when setting this, the buffer
    size is limited by net.core.wmem_max\n# and when not setting it it is defined
    by net.ipv4.tcp_wmem\n# internode_recv_buff_size_in_bytes:\n\n# Frame size for
    thrift (maximum message length).\nthrift_framed_transport_size_in_mb: 15\n\n#
    Set to true to have Cassandra create a hard link to each sstable\n# flushed or
    streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing
    these links is the operator's\n# responsibility.\nincremental_backups: false\n\n#
    Whether or not to take a snapshot before each compaction.  Be\n# careful using
    this option, since Cassandra won't clean up the\n# snapshots for you.  Mostly
    useful if you're paranoid when there\n# is a data format change.\nsnapshot_before_compaction:
    false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n#
    or dropping of column families. The STRONGLY advised default of true \n# should
    be used to provide data safety. If you set this flag to false, you will\n# lose
    data on truncation or drop.\nauto_snapshot: true\n\n# Granularity of the collation
    index of rows within a partition.\n# Increase if your rows are large, or if you
    have a very large\n# number of rows per partition.  The competing goals are these:\n#\n#
    - a smaller granularity means more index entries are generated\n#   and looking
    up rows withing the partition by collation column\n#   is faster\n# - but, Cassandra
    will keep the collation index in memory for hot\n#   rows (as part of the key
    cache), so a larger granularity means\n#   you can cache more hot rows\ncolumn_index_size_in_kb:
    64\n\n# Per sstable indexed key cache entries (the collation index in memory\n#
    mentioned above) exceeding this size will not be held on heap.\n# This means that
    only partition information is held on heap and the\n# index entries are read from
    disk.\n#\n# Note that this size refers to the size of the\n# serialized index
    information and not the size of the partition.\ncolumn_index_cache_size_in_kb:
    2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation
    \"compactions\" for anti-entropy repair.  Simultaneous\n# compactions can help
    preserve read performance in a mixed read/write\n# workload, by mitigating the
    tendency of small sstables to accumulate\n# during a single long running compactions.
    The default is usually\n# fine and if you experience problems with compaction
    running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec
    first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n#
    number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data
    directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors:
    1\n\n# Throttles compaction to the given total throughput across the entire\n#
    system. The faster you insert data, the faster you need to compact in\n# order
    to keep the sstable count down, but in general, setting this to\n# 16 to 32 times
    the rate you are inserting data is more than sufficient.\n# Setting this to 0
    disables throttling. Note that this account for all types\n# of compaction, including
    validation compaction.\ncompaction_throughput_mb_per_sec: 16\n\n# When compacting,
    the replacement sstable(s) can be opened before they\n# are completely written,
    and used in place of the prior sstables for\n# any range that has been written.
    This helps to smoothly transfer reads \n# between the sstables, reducing page
    cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb:
    50\n\n# Throttles all outbound streaming file transfers on this node to the\n#
    given total throughput in Mbps. This is necessary because Cassandra does\n# mostly
    sequential IO when streaming data during bootstrap or repair, which\n# can lead
    to saturating the network connection and degrading rpc performance.\n# When unset,
    the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec:
    200\n\n# Throttles all streaming file transfer between the datacenters,\n# this
    setting allows users to throttle inter dc stream throughput in addition\n# to
    throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n#
    When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec:
    200\n\n# How long the coordinator should wait for read operations to complete\nread_request_timeout_in_ms:
    5000\n# How long the coordinator should wait for seq or index scans to complete\nrange_request_timeout_in_ms:
    10000\n# How long the coordinator should wait for writes to complete\nwrite_request_timeout_in_ms:
    2000\n# How long the coordinator should wait for counter writes to complete\ncounter_write_request_timeout_in_ms:
    5000\n# How long a coordinator should continue to retry a CAS operation\n# that
    contends with other proposals for the same row\ncas_contention_timeout_in_ms:
    1000\n# How long the coordinator should wait for truncates to complete\n# (This
    can be much longer, because unless auto_snapshot is disabled\n# we need to flush
    first so we can snapshot before removing the data.)\ntruncate_request_timeout_in_ms:
    60000\n# The default timeout for other, miscellaneous operations\nrequest_timeout_in_ms:
    10000\n\n# How long before a node logs slow queries. Select queries that take
    longer than\n# this timeout to execute, will generate an aggregated log message,
    so that slow queries\n# can be identified. Set this value to zero to disable slow
    query logging.\nslow_query_log_timeout_in_ms: 500\n\n# Enable operation timeout
    information exchange between nodes to accurately\n# measure request timeouts.
    \ If disabled, replicas will assume that requests\n# were forwarded to them instantly
    by the coordinator, which means that\n# under overload conditions we will waste
    that much extra time processing \n# already-timed-out requests.\n#\n# Warning:
    before enabling this property make sure to ntp is installed\n# and the times are
    synchronized between the nodes.\ncross_node_timeout: false\n\n# Set keep-alive
    period for streaming\n# This node will send a keep-alive message periodically
    with this period.\n# If the node does not receive a keep-alive message from the
    peer for\n# 2 keep-alive cycles the stream session times out and fail\n# Default
    value is 300s (5 minutes), which means stalled stream\n# times out in 10 minutes
    by default\n# streaming_keep_alive_period_in_secs: 300\n\n# phi value that must
    be reached for a host to be marked down.\n# most users should never need to adjust
    this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class
    that implements\n# IEndpointSnitch.  The snitch has two functions:\n#\n# - it
    teaches Cassandra enough about your network topology to route\n#   requests efficiently\n#
    - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated
    failures. It does this by grouping machines into\n#   \"datacenters\" and \"racks.\"
    \ Cassandra will do its best not to have\n#   more than one replica on the same
    \"rack\" (which may not actually\n#   be a physical location)\n#\n# CASSANDRA
    WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\n# ONCE DATA IS INSERTED
    INTO THE CLUSTER.  This would cause data loss.\n# This means that if you start
    with the default SimpleSnitch, which\n# locates every node on \"rack1\" in \"datacenter1\",
    your only options\n# if you need to add another datacenter are GossipingPropertyFileSnitch\n#
    (and the older PFS).  From there, if you want to migrate to an\n# incompatible
    snitch like Ec2Snitch you can do it by adding new nodes\n# under Ec2Snitch (which
    will locate them in a new \"datacenter\") and\n# decommissioning the old ones.\n#\n#
    Out of the box, Cassandra provides:\n#\n# SimpleSnitch:\n#    Treats Strategy
    order as proximity. This can improve cache\n#    locality when disabling read
    repair.  Only appropriate for\n#    single-datacenter deployments.\n#\n# GossipingPropertyFileSnitch\n#
    \   This should be your go-to snitch for production use.  The rack\n#    and datacenter
    for the local node are defined in\n#    cassandra-rackdc.properties and propagated
    to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it
    is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#\n#
    PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which
    are\n#    explicitly configured in cassandra-topology.properties.\n#\n# Ec2Snitch:\n#
    \   Appropriate for EC2 deployments in a single Region. Loads Region\n#    and
    Availability Zone information from the EC2 API. The Region is\n#    treated as
    the datacenter, and the Availability Zone as the rack.\n#    Only private IPs
    are used, so this will not work across multiple\n#    Regions.\n#\n# Ec2MultiRegionSnitch:\n#
    \   Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.
    \ (Thus, you should set seed addresses to the public\n#    IP as well.) You will
    need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.
    \ (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#
    \   establishing a connection.)\n#\n# RackInferringSnitch:\n#    Proximity is
    determined by rack and data center, which are\n#    assumed to correspond to the
    3rd and 2nd octet of each node's IP\n#    address, respectively.  Unless this
    happens to match your\n#    deployment conventions, this is best used as an example
    of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You
    can use a custom Snitch by setting this to the full class name\n# of the snitch,
    which will be assumed to be on your classpath.\nendpoint_snitch: SimpleSnitch\n\n#
    controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms:
    100 \n# controls how often to reset all host scores, allowing a bad host to\n#
    possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater
    than zero and read_repair_chance is < 1.0, this will allow\n# 'pinning' of replicas
    to hosts in order to increase cache capacity.\n# The badness threshold will control
    how much worse the pinned host has to be\n# before the dynamic snitch will prefer
    other replicas over it.  This is\n# expressed as a double which represents a percentage.
    \ Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static
    snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold:
    0.1\n\n# request_scheduler -- Set this to a class that implements\n# RequestScheduler,
    which will schedule incoming client requests\n# according to the specific policy.
    This is useful for multi-tenancy\n# with a single Cassandra cluster.\n# NOTE:
    This is specifically for requests from the client and does\n# not affect inter
    node communication.\n# org.apache.cassandra.scheduler.NoScheduler - No scheduling
    takes place\n# org.apache.cassandra.scheduler.RoundRobinScheduler - Round robin
    of\n# client requests to a node with a separate queue for each\n# request_scheduler_id.
    The scheduler is further customized by\n# request_scheduler_options as described
    below.\nrequest_scheduler: org.apache.cassandra.scheduler.NoScheduler\n\n# Scheduler
    Options vary based on the type of scheduler\n#\n# NoScheduler\n#   Has no options\n#\n#
    RoundRobin\n#   throttle_limit\n#     The throttle_limit is the number of in-flight\n#
    \    requests per client.  Requests beyond \n#     that limit are queued up until\n#
    \    running requests can complete.\n#     The value of 80 here is twice the number
    of\n#     concurrent_reads + concurrent_writes.\n#   default_weight\n#     default_weight
    is optional and allows for\n#     overriding the default which is 1.\n#   weights\n#
    \    Weights are optional and will default to 1 or the\n#     overridden default_weight.
    The weight translates into how\n#     many requests are handled during each turn
    of the\n#     RoundRobin, based on the scheduler id.\n#\n# request_scheduler_options:\n#
    \   throttle_limit: 80\n#    default_weight: 5\n#    weights:\n#      Keyspace1:
    1\n#      Keyspace2: 5\n\n# request_scheduler_id -- An identifier based on which
    to perform\n# the request scheduling. Currently the only valid option is keyspace.\n#
    request_scheduler_id: keyspace\n\n# Enable or disable inter-node encryption\n#
    JVM defaults for supported SSL socket protocols and cipher suites can\n# be replaced
    using custom encryption options. This is not recommended\n# unless you have policies
    in place that dictate certain settings, or\n# need to disable vulnerable ciphers
    or protocols in case the JVM cannot\n# be updated.\n# FIPS compliant settings
    can be configured at JVM level and should not\n# involve changing encryption settings
    here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n#
    *NOTE* No custom encryption options are enabled at the moment\n# The available
    internode options are : all, none, dc, rack\n#\n# If set to dc cassandra will
    encrypt the traffic between the DCs\n# If set to rack cassandra will encrypt the
    traffic between the racks\n#\n# The passwords used in these options must match
    the passwords used when generating\n# the keystore and truststore.  For instructions
    on generating these files, see:\n# http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n#\nserver_encryption_options:\n
    \   internode_encryption: none\n    keystore: conf/.keystore\n    keystore_password:
    cassandra\n    truststore: conf/.truststore\n    truststore_password: cassandra\n
    \   # More advanced defaults below:\n    # protocol: TLS\n    # algorithm: SunX509\n
    \   # store_type: JKS\n    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n
    \   # require_client_auth: false\n    # require_endpoint_verification: false\n\n#
    enable or disable client/server encryption.\nclient_encryption_options:\n    enabled:
    false\n    # If enabled and optional is set to true encrypted and unencrypted
    connections are handled.\n    optional: false\n    keystore: conf/.keystore\n
    \   keystore_password: cassandra\n    # require_client_auth: false\n    # Set
    trustore and truststore_password if require_client_auth is true\n    # truststore:
    conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults
    below:\n    # protocol: TLS\n    # algorithm: SunX509\n    # store_type: JKS\n
    \   # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n\n#
    internode_compression controls whether traffic between nodes is\n# compressed.\n#
    Can be:\n#\n# all\n#   all traffic is compressed\n#\n# dc\n#   traffic between
    different datacenters is compressed\n#\n# none\n#   nothing is compressed.\ninternode_compression:
    dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling
    it will result in larger (but fewer) network packets being sent,\n# reducing overhead
    from the TCP protocol itself, at the cost of increasing\n# latency if you block
    for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different
    trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl:
    604800\n\n# By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\n#
    This threshold can be adjusted to minimize logging if necessary\n# gc_log_threshold_in_ms:
    200\n\n# If unset, all GC Pauses greater than gc_log_threshold_in_ms will log
    at\n# INFO level\n# UDFs (user defined functions) are disabled by default.\n#
    As of Cassandra 3.0 there is a sandbox in place that should prevent execution
    of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs
    (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions
    is true.\n# Enable this option to be able to use UDFs with \"language javascript\"
    or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions
    is false.\nenable_scripted_user_defined_functions: false\n\n# Enables materialized
    view creation on this node.\n# Materialized views are considered experimental
    and are not recommended for production use.\nenable_materialized_views: true\n\n#
    The default Windows kernel timer and scheduling resolution is 15.6ms for power
    conservation.\n# Lowering this value on Windows can provide much tighter latency
    and better throughput, however\n# some virtualized environments may see a negative
    performance impact from changing this setting\n# below their system default. The
    sysinternals 'clockres' tool can confirm your system's default\n# setting.\nwindows_timer_interval:
    1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can
    be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore
    can hold multiple keys, but the one referenced by\n# the \"key_alias\" is the
    only key that will be used for encrypt opertaions; previously used keys\n# can
    still (and should!) be in the keystore and will be used on decrypt operations\n#
    (to handle the case of key rotation).\n#\n# It is strongly recommended to download
    and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction
    Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n#
    Currently, only the following file types are supported for transparent data encryption,
    although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n
    \   enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n
    \   key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which
    is also the default size)\n    # iv_length: 16\n    key_provider: \n      - class_name:
    org.apache.cassandra.security.JKSKeyProvider\n        parameters: \n          -
    keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type:
    JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY
    THRESHOLDS #\n#####################\n\n# When executing a scan, within or across
    a partition, we need to keep the\n# tombstones seen in memory so we can return
    them to the coordinator, which\n# will use them to make sure other replicas also
    know about the deleted rows.\n# With workloads that generate a lot of tombstones,
    this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n#
    Adjust the thresholds here if you understand the dangers and want to\n# scan more
    tombstones anyway.  These thresholds may also be adjusted at runtime\n# using
    the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold:
    100000\n\n# Log WARN on any multiple-partition batch size exceeding this value.
    5kb per batch by default.\n# Caution should be taken on increasing the size of
    this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb:
    5\n\n# Fail any multiple-partition batch exceeding this value. 50kb (10x warn
    threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any
    batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold:
    10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb:
    100\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN
    level\n# Adjust the threshold based on your application throughput requirement\n#
    By default, Cassandra logs GC Pauses greater than 200 ms at INFO level\ngc_warn_threshold_in_ms:
    1000\n\n# Maximum size of any value in SSTables. Safety measure to detect SSTable
    corruption\n# early. Any value size larger than this threshold will result into
    marking an SSTable\n# as corrupted. This should be positive and less than 2048.\n#
    max_value_size_in_mb: 256\n\n# Back-pressure settings #\n# If enabled, the coordinator
    will apply the back-pressure strategy specified below to each mutation\n# sent
    to replicas, with the aim of reducing pressure on overloaded replicas.\nback_pressure_enabled:
    false\n# The back-pressure strategy applied.\n# The default implementation, RateBasedBackPressure,
    takes three arguments:\n# high ratio, factor, and flow type, and uses the ratio
    between incoming mutation responses and outgoing mutation requests.\n# If below
    high ratio, outgoing mutations are rate limited according to the incoming rate
    decreased by the given factor;\n# if above high ratio, the rate limiting is increased
    by the given factor;\n# such factor is usually best configured between 1 and 10,
    use larger values for a faster recovery\n# at the expense of potentially more
    dropped mutations;\n# the rate limiting is applied according to the flow type:
    if FAST, it's rate limited at the speed of the fastest replica,\n# if SLOW at
    the speed of the slowest one.\n# New strategies can be added. Implementors need
    to implement org.apache.cassandra.net.BackpressureStrategy and\n# provide a public
    constructor accepting a Map<String, Object>.\nback_pressure_strategy:\n    - class_name:
    org.apache.cassandra.net.RateBasedBackPressure\n      parameters:\n        - high_ratio:
    0.90\n          factor: 5\n          flow: FAST\n\n# Coalescing Strategies #\n#
    Coalescing multiples messages turns out to significantly boost message processing
    throughput (think doubling or more).\n# On bare metal, the floor for packet processing
    throughput is high enough that many applications won't notice, but in\n# virtualized
    environments, the point at which an application can be bound by network packet
    processing can be\n# surprisingly low compared to the throughput of task processing
    that is possible inside a VM. It's not that bare metal\n# doesn't benefit from
    coalescing messages, it's that the number of packets a bare metal network interface
    can process\n# is sufficient for many applications such that no load starvation
    is experienced even without coalescing.\n# There are other benefits to coalescing
    network messages that are harder to isolate with a simple metric like messages\n#
    per second. By coalescing multiple tasks together, a network thread can process
    multiple messages for the cost of one\n# trip to read from a socket, and all the
    task submission work can be done at the same time reducing context switching\n#
    and increasing cache friendliness of network message processing.\n# See CASSANDRA-8692
    for details.\n\n# Strategy to use for coalescing messages in OutboundTcpConnection.\n#
    Can be fixed, movingaverage, timehorizon, disabled (default).\n# You can also
    specify a subclass of CoalescingStrategies.CoalescingStrategy by name.\n# otc_coalescing_strategy:
    DISABLED\n\n# How many microseconds to wait for coalescing. For fixed strategy
    this is the amount of time after the first\n# message is received before it will
    be sent with any accompanying messages. For moving average this is the\n# maximum
    amount of time that will be waited as well as the interval at which messages must
    arrive on average\n# for coalescing to be enabled.\n# otc_coalescing_window_us:
    200\n\n# Do not try to coalesce messages if we already got that many messages.
    This should be more than 2 and less than 128.\n# otc_coalescing_enough_coalesced_messages:
    8\n\n# How many milliseconds to wait between two expiration runs on the backlog
    (queue) of the OutboundTcpConnection.\n# Expiration is done if messages are piling
    up in the backlog. Droppable messages are expired to free the memory\n# taken
    by expired messages. The interval should be between 0 and 1000, and in most installations
    the default value\n# will be appropriate. A smaller value could potentially expire
    messages slightly sooner at the expense of more CPU\n# time and queue contention
    while iterating the backlog of messages.\n# An interval of 0 disables any wait
    time, which is the behavior of former Cassandra versions.\n#\n# otc_backlog_expiration_interval_ms:
    200\n"
